# Error model: 15% error rate, DSTC2 confscorer, DSTC2 nbestgenerator
# User model: standard sampled params, sampled patience
# Masks: off

###### General parameters ######
[GENERAL]
# Set to "SFRestaurants" or "Laptops11"
domains = CamRestaurants
singledomain = True
tracedialog = 0
seed = 0

[exec_config]
configdir = _benchmarkpolicies/env1-feudalgain
logfiledir = _benchmarklogs/env1-feudalgain
numtrainbatches = 20
traindialogsperbatch = 200
numbatchtestdialogs =  500
trainsourceiteration = 0
numtestdialogs =  500
trainerrorrate = 0
testerrorrate  = 0
testeverybatch = True
deleteprevpolicy = True

[logging]
usecolor = False
screen_level = results
file_level = results
file = auto

###### Environment parameters ######

[agent]
maxturns = 25

[usermodel]
usenewgoalscenarios = True
oldstylepatience = False
patience = 4,6
configfile = config/sampledUM.cfg

[errormodel]
nbestsize = 1
confusionmodel = RandomConfusions
nbestgeneratormodel = SampledNBestGenerator
confscorer = additive

[summaryacts]
maxinformslots = 5
informmask = True
requestmask = True
informcountaccepted = 4
byemask = True

###### Dialogue Manager parameters ######
[policy]
policydir = _benchmarkpolicies/env1-feudalgain
belieftype = focus
useconfreq = False
learning = True
policytype = feudalgain
startwithhello = False
inpolicyfile = auto
outpolicyfile = auto
# Set noisy_acer=False for vanilla neural networks
noisy_acer = True
# Set use_pass=True if transitions where pass() action were taken should be used. Always False if InformationGain is used.
use_pass = False

[feudalpolicy]
features=learned
si_policy_type=acer
# only_master=True means that we use only policy pi_mg, set to False if you want to use pi_m and pi_g
only_master = True
# Set the threshold for information gain reward calculated by JS-divergence. If set to 1.0, we do not use InformationGain.
js_threshold = 0.2

[dqnpolicy]
q_update = double
# set architecture=duel for vanilla neural networks
architecture = noisy_duel
h1_size = 300
h2_size = 100
capacity = 2000
beta = 0.95
epsilon_start = 0.3
maxiter = 4000
minibatch_size = 64
is_threshold = 5.0
episodeNum = 0.0
epsilon_end = 0.0
n_in = 268
features = ["discourseAct", "method", "requested", "full", "lastActionInformNone", "offerHappened", "inform_info"]

###### Evaluation parameters ######

[eval]
rewardvenuerecommended=0
penaliseallturns = True
wrongvenuepenalty = 0
notmentionedvaluepenalty = 0
successmeasure = objective
successreward = 20

